#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""Helpers for expanding PNF generic name aliases and special abbreviations."""
from __future__ import annotations

import re
from typing import Set

import unicodedata

from .text_utils import _base_name

_CONNECTOR_PATTERN = re.compile(r"\s*(?:\+|/|&|,| and | plus )\s*", re.IGNORECASE)
_PAREN_RX = re.compile(r"\(([^)]+)\)")

SALT_FORM_SUFFIXES = {
    "hydrochloride",
    "hcl",
    "sodium",
    "potassium",
    "magnesium",
    "calcium",
    "sr",
    "xr",
    "er",
    "cr",
    "cap",
    "caps",
    "tab",
    "tabs",
    "inj",
    "inj.",
    "ophth",
    "otc",
}

SPECIAL_GENERIC_ALIASES: dict[str, Set[str]] = {
    "aluminum_magnesium": {"almg", "almag", "aloh mgoh", "aloh mgo h", "alohmgoh", "aloh + mgoh"},
    "anti_tetanus_serum": {"ats"},
    "penicillin_g_benzylpenicillin_crystalline": {"pen g", "pen-g", "peng"},
    "isosorbide": {"ismn", "isdn"},
}

SPELLING_RULES = [
    (re.compile(r"ceph"), "cef"),
    (re.compile(r"sulph"), "sulf"),
    (re.compile(r"oes"), "es"),
    (re.compile(r"oest"), "est"),
    (re.compile(r"haem"), "hem"),
    (re.compile(r"oed"), "ed"),
    (re.compile(r"paedi"), "pedi"),
    (re.compile(r"anaem"), "anem"),
    (re.compile(r"aluminium"), "aluminum"),
    (re.compile(r"amoxycill"), "amoxicill"),
    (re.compile(r"(?<=ce)ph"), "f"),
]


def _ascii_fold(text: str) -> str:
    return "".join(c for c in unicodedata.normalize("NFKD", text) if not unicodedata.combining(c))


def _remove_salt_suffix(tokens: list[str]) -> list[str]:
    trimmed = list(tokens)
    while trimmed and trimmed[-1] in SALT_FORM_SUFFIXES:
        trimmed.pop()
    return trimmed


def _is_valid_alias(text: str) -> bool:
    """Filter out aliases that are too short or look like doses/units."""
    if not text:
        return False
    stripped = text.strip().lower()
    if not stripped:
        return False
    letters = sum(1 for ch in stripped if ch.isalpha())
    if letters < 3:
        return False
    if stripped[0].isdigit():
        return False
    return True


def apply_spelling_rules(text: str) -> Set[str]:
    """Return variants generated by sequential spelling-normalization rules."""
    variants: Set[str] = {text}
    for pattern, replacement in SPELLING_RULES:
        new_variants: Set[str] = set()
        for variant in variants:
            replaced = pattern.sub(replacement, variant)
            if replaced != variant:
                new_variants.add(replaced)
        variants |= new_variants
    return {v for v in variants if v}


def _strip_parentheses(text: str) -> str:
    """Remove parenthetical segments while retaining surrounding spacing."""
    return _PAREN_RX.sub(" ", text)


def expand_generic_aliases(name: str) -> Set[str]:
    """Generate a set of lowercase alias strings for a PNF generic name."""
    variants: Set[str] = set()
    if not isinstance(name, str):
        return variants
    raw = name.strip()
    if not raw:
        return variants

    # Start with raw name and base form.
    variants.add(raw)
    base = _base_name(raw)
    variants.add(base)

    # Capture individual parenthetical entries (e.g., Co-amoxiclav).
    for par in _PAREN_RX.findall(raw):
        par = par.strip()
        if par:
            variants.add(par)

    # Remove parentheses content entirely.
    no_paren = _strip_parentheses(raw)
    variants.add(no_paren)

    # Normalize hyphen spacing variations.
    variants.add(raw.replace("-", " "))

    generated: Set[str] = set()
    for variant in list(variants):
        if not variant:
            continue
        ascii_variant = _ascii_fold(variant)
        ascii_variant = re.sub(r"\s+", " ", ascii_variant.replace("-", " ")).strip()
        if not ascii_variant:
            continue
        pieces = [p.strip().lower() for p in _CONNECTOR_PATTERN.split(ascii_variant) if p.strip()]
        if not pieces:
            continue
        stripped = _remove_salt_suffix(pieces)
        if not stripped:
            stripped = pieces
        joined = " ".join(stripped)
        generated.add(joined)
        generated.add("".join(stripped))
        if len(stripped) > 1:
            tokens_sorted = sorted(stripped)
            generated.add(" ".join(tokens_sorted))
            generated.add("".join(tokens_sorted))
        for piece in stripped:
            generated.add(piece)

    cleaned: Set[str] = set()
    for variant in generated:
        if not variant:
            continue
        base_norm = re.sub(r"\s+", " ", variant).strip().lower()
        if not base_norm:
            continue
        def _add(item: str) -> None:
            if item and _is_valid_alias(item):
                cleaned.add(item)
        _add(base_norm)
        compact = base_norm.replace(" ", "")
        if compact:
            _add(compact)
        for ruled in apply_spelling_rules(base_norm):
            ruled_norm = re.sub(r"\s+", " ", ruled).strip()
            if ruled_norm:
                _add(ruled_norm)
                compact_ruled = ruled_norm.replace(" ", "")
                if compact_ruled:
                    _add(compact_ruled)

    return cleaned
